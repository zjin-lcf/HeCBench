// a modified reference generated by Gemini

#include <cmath>
#include <vector>
#include <algorithm>

template<typename T>
struct PagedAttentionParams {
    T* out;                // [num_seqs, num_query_heads, head_size]
    const T* query;        // [num_seqs, num_query_heads, head_size]
    const T* key_cache;    // [num_blocks, num_kv_heads, head_size/x, block_size, x]
    const T* value_cache;  // [num_blocks, num_kv_heads, head_size, block_size]
    const int* block_tables; 
    const int* seq_lens;
    const float* alibi_slopes;
    
    int num_seqs;
    int num_query_heads;
    int num_kv_heads;
    int head_size;
    int block_size;
    int max_num_blocks_per_seq;
    float scale;
};

void softmax(float* scores, int length) {
    float max_val = -INFINITY;
    for (int i = 0; i < length; ++i) max_val = std::max(max_val, scores[i]);
    float sum = 0.0f;
    for (int i = 0; i < length; ++i) {
        scores[i] = std::exp(scores[i] - max_val);
        sum += scores[i];
    }
    for (int i = 0; i < length; ++i) scores[i] /= sum;
}

template<typename T>
void reference(PagedAttentionParams<T> params) {
    int queries_per_kv = params.num_query_heads / params.num_kv_heads;

    for (int64_t i = 0; i < params.num_seqs; ++i) {
        int seq_len = params.seq_lens[i];
        const int* block_table = params.block_tables + i * params.max_num_blocks_per_seq;

        for (int h = 0; h < params.num_query_heads; ++h) {
            int kv_h = h / queries_per_kv;
            std::vector<float> logits(seq_len);

            // 1. Compute Dot Product (Q * K) + ALiBi
            for (int t = 0; t < seq_len; ++t) {
                int64_t block_idx = block_table[t / params.block_size];
                int block_offset = t % params.block_size;

                float score = 0.0f;
                for (int d = 0; d < params.head_size; ++d) {
                    // Accessing Key Cache [block, head, dim, offset]
                    // Note: Layout depends on specific PagedAttention implementation
                    T q_val = params.query[i * params.num_query_heads * params.head_size + h * params.head_size + d];
                    T k_val = params.key_cache[block_idx * params.num_kv_heads * params.head_size * params.block_size + 
                                               kv_h * params.head_size * params.block_size + 
                                               d * params.block_size + block_offset];
                    score += (float)q_val * (float)k_val;
                }
                score *= params.scale;

                // Apply ALiBi Bias
                if (params.alibi_slopes != nullptr) {
                    float slope = params.alibi_slopes[h];
                    float bias = slope * (float)(t - seq_len + 1);
                    score += bias;
                }
                logits[t] = score;
            }

            // 2. Softmax
            softmax(logits.data(), seq_len);

            // 3. Weighted Sum (Logits * V)
            for (int d = 0; d < params.head_size; ++d) {
                float res = 0.0f;
                for (int t = 0; t < seq_len; ++t) {
                    int64_t block_idx = block_table[t / params.block_size];
                    int block_offset = t % params.block_size;

                    T v_val = params.value_cache[block_idx * params.num_kv_heads * params.head_size * params.block_size + 
                                                 kv_h * params.head_size * params.block_size + 
                                                 d * params.block_size + block_offset];
                    res += logits[t] * (float)v_val;
                }
                params.out[i * params.num_query_heads * params.head_size + h * params.head_size + d] = (T)res;
            }
        }
    }
}
