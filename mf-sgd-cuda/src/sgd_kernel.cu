#include "sgd.h"
#include "sgd_kernel.h"
#include <curand.h>
#include <curand_kernel.h>

using namespace std;

__device__ unsigned int update_count;

extern __global__ void init_rand_state(curandState*state, int size);
extern __global__ void init_block_lock(bool*row, bool*col, int b);


#include "sgd_k128_kernel_hogwild_warp32.h"


__global__ void init_rand_state(curandState*state, int size)
{
  int tid = blockIdx.x*blockDim.x + threadIdx.x;
  if(tid < size) curand_init(tid,tid,0,&state[tid]);
}


__global__ void transform_half(half *gpu_half_feature, float *gpu_float_feature, long long vec_size)
{
  int tid = blockIdx.x*blockDim.x + threadIdx.x;
  int number_threads = gridDim.x*blockDim.x;

  for(long long i = tid;i < vec_size;i += number_threads)
  {
    gpu_float_feature[i] = __half2float(gpu_half_feature[i]); 
  }
}

void transform_feature_vector(short *half_feature, float *float_feature, int m, int grid, long long seg, int k)
{

  half *gpu_half_feature;
  float *gpu_float_feature;

  cudaMalloc((void**)&gpu_half_feature, sizeof(half)*seg*k);
  cudaMalloc((void**)&gpu_float_feature, sizeof(float)*seg*k);
  gpuErr(cudaPeekAtLastError());

  for(int i = 0;i < grid;i++)
  {
    cudaMemcpy(gpu_half_feature, half_feature + i*seg*k, sizeof(half)*seg*k, cudaMemcpyHostToDevice);
    gpuErr(cudaPeekAtLastError());

    int num_blocks = (seg*k+255)/256;
    if(num_blocks > 8*24)num_blocks = 8*24;

    transform_half<<<num_blocks,256>>>(gpu_half_feature, gpu_float_feature, seg*k);

    gpuErr(cudaPeekAtLastError());
    cudaMemcpy(float_feature + i*seg*k, gpu_float_feature, sizeof(float)*seg*k, cudaMemcpyDeviceToHost);
    gpuErr(cudaPeekAtLastError());
  }

  cudaFree(gpu_half_feature);
  cudaFree(gpu_float_feature);
  gpuErr(cudaPeekAtLastError());
}

__global__ void pre_transform_print(half *p, half *q)
{
  long long m_index, n_index;

  m_index = 0;
  printf("m:%lld\n", m_index);
  for(long long i = m_index*128; i < m_index*128 + 128; i++)
  {
    printf("%.6f ", __half2float(p[i]));
  }
  printf("\n");

  m_index = 1;
  printf("m:%lld\n", m_index);
  for(long long i = m_index*128; i < m_index*128 + 128; i++)
  {
    printf("%.6f ", __half2float(p[i]));
  }
  printf("\n");

  m_index = 2;
  printf("m:%lld\n", m_index);
  for(long long i = m_index*128; i < m_index*128 + 128; i++)
  {
    printf("%.6f ", __half2float(p[i]));
  }
  printf("\n");


  // n
  n_index = 0;
  printf("n:%lld\n", n_index);
  for(long long i = n_index*128; i < n_index*128 + 128; i++)
  {
    printf("%.6f ", __half2float(q[i]));
  }
  printf("\n");

  n_index = 1;
  printf("n:%lld\n", n_index);
  for(long long i = n_index*128; i < n_index*128 + 128; i++)
  {
    printf("%.6f ", __half2float(q[i]));
  }
  printf("\n");

  n_index = 2;
  printf("n:%lld\n", n_index);
  for(long long i = n_index*128; i < n_index*128 + 128; i++)
  {
    printf("%.6f ", __half2float(q[i]));
  }
  printf("\n");

}
void sgd_update_k128(Parameter para, mf_model *model, mf_problem *prob, float scale)
{
  printf("calling sgd_update_k128() ...\n");
  //generate the random state for the hogwild scheduling policy.
  curandState *rand_state;
  cudaMalloc((void**)&rand_state, sizeof(curandState)*para.num_workers);
  gpuErr(cudaPeekAtLastError());

  init_rand_state<<<((para.num_workers+255)/256),256>>>(rand_state,para.num_workers);
  gpuErr(cudaPeekAtLastError());

  //generate the dynamic learning rate
  float dynamic_rate[1024];
  float alpha = para.alpha;
  float beta  = para.beta;
  float lrate = para.lrate;

  for(int i = 0;i < (para.num_iters + 4);i++)
  {
    double tmp_rate = alpha/(1 + beta*pow(i, 1.5)) + lrate;
    dynamic_rate[i] = tmp_rate;
  }
  float *gpu_dynamic_rate;
  cudaMalloc((void**)&gpu_dynamic_rate, sizeof(float)*1024);
  gpuErr(cudaPeekAtLastError());
  cudaMemcpy(gpu_dynamic_rate, dynamic_rate, sizeof(float)*1024, cudaMemcpyHostToDevice);
  gpuErr(cudaPeekAtLastError());

  fflush(stdout);

  //malloc a problem grid on GPU
  if(prob->x_grid*prob->y_grid == 1)
  {
    cudaMalloc((void**)&(prob->gpuR), sizeof(mf_node)*prob->maxGridSize);
    prob->cur_u_id = -1;
    prob->cur_v_id = -1;
  }
  else
  {
    cudaMalloc((void**)&(prob->gpuRptrs[0]), sizeof(mf_node)*prob->maxGridSize);
    cudaMalloc((void**)&(prob->gpuRptrs[1]), sizeof(mf_node)*prob->maxGridSize);
    prob->cur_global_x_id[0] = -1;
    prob->cur_global_x_id[1] = -1;
    prob->cur_global_y_id[0] = -1;
    prob->cur_global_y_id[1] = -1;
  }

  //malloc feature vectors on GPU
  if(prob->x_grid*prob->y_grid == 1)
  {
    cudaMalloc((void**)&model->gpuHalfp, sizeof(half)*model->u_seg*model->k);
    cudaMalloc((void**)&model->gpuHalfq, sizeof(half)*model->v_seg*model->k);
    model->cur_u_id = -1;
    model->cur_v_id = -1;
  }
  else
  {
    cudaMalloc((void**)&model->gpuHalfPptrs[0], sizeof(half)*model->u_seg*model->k);
    cudaMalloc((void**)&model->gpuHalfPptrs[1], sizeof(half)*model->u_seg*model->k);
    cudaMalloc((void**)&model->gpuHalfQptrs[0], sizeof(half)*model->v_seg*model->k);
    cudaMalloc((void**)&model->gpuHalfQptrs[1], sizeof(half)*model->v_seg*model->k);

    model->cur_global_x_id[0] = -1;
    model->cur_global_x_id[1] = -1;
    model->cur_global_y_id[0] = -1;
    model->cur_global_y_id[1] = -1;
  }   

  //set update count
  int update_vector_size = 128;
  int *update_count_per_block = new int[prob->ux*prob->vy]();
  int max_update_count_per_block = -1;
  for(int cur_grid_id = 0;cur_grid_id < prob->ux*prob->vy; cur_grid_id ++)
  {
    update_count_per_block[cur_grid_id] = (ceil)(1.0*prob->gridSize[cur_grid_id]/(para.num_workers*update_vector_size));   
    if(max_update_count_per_block < update_count_per_block[cur_grid_id])
    {
      max_update_count_per_block = update_count_per_block[cur_grid_id];
    }
  }

  // random shuffle
  random_device rd;
  mt19937 g(rd());

  //run the update kernel
  if(prob->u_grid*prob->v_grid == 1)
  {
    cudaMemcpy(prob->gpuR, prob->R2D[0], sizeof(mf_node)*prob->gridSize[0], cudaMemcpyHostToDevice);
    cudaMemcpy(model->gpuHalfp, model->halfp, sizeof(half)*model->u_seg*model->k, cudaMemcpyHostToDevice);
    cudaMemcpy(model->gpuHalfq, model->halfq, sizeof(half)*model->v_seg*model->k, cudaMemcpyHostToDevice);

    clock_t start = clock();

    sgd_k128_kernel_hogwild_warp32_lrate<<<para.num_workers/4,128>>>(
        prob->gpuR,
        prob->gridSize[0],
        model->gpuHalfp,
        model->gpuHalfq,
        rand_state,
        gpu_dynamic_rate,
        model->u_seg,
        model->v_seg,
        model->k,
        para.num_iters,
        0,
        max_update_count_per_block,
        update_count_per_block[0],
        update_vector_size,
        para.lambda_p,
        para.lambda_q,
        prob->u_grid,
        prob->v_grid,
        0,
        0);
    cudaDeviceSynchronize();
    double time_ela = (clock()-start)/double(CLOCKS_PER_SEC);
    printf("update_per_sec:%f\n", prob->nnz*para.num_iters/time_ela);

    cudaMemcpy(model->halfp, model->gpuHalfp, sizeof(half)*model->u_seg*model->k, cudaMemcpyDeviceToHost);
    cudaMemcpy(model->halfq, model->gpuHalfq, sizeof(half)*model->v_seg*model->k, cudaMemcpyDeviceToHost);
  }
  else if(prob->x_grid*prob->y_grid == 1)
  {
    clock_t start = clock();

    //random shuffle
    vector<int> u_id_vec(prob->u_grid, 0);
    vector<int> v_id_vec(prob->v_grid, 0);
    for(int i = 0;i < prob->u_grid;i++) u_id_vec[i] = i;
    for(int i = 0;i < prob->v_grid;i++) v_id_vec[i] = i;

    for(int iter = 0;iter < para.num_iters; iter ++)
    {
      clock_t ite_start = clock();

      shuffle(u_id_vec.begin(), u_id_vec.end(), g);
      for(int u_ite = 0;u_ite < prob->u_grid; u_ite ++)
      {

        shuffle(v_id_vec.begin(), v_id_vec.end(), g);
        for(int v_ite = 0;v_ite < prob->v_grid; v_ite ++)
        {
          int cur_u_id = u_id_vec[u_ite];
          int cur_v_id = v_id_vec[v_ite];

          int cur_grid_id = cur_u_id*prob->v_grid + cur_v_id;
          //transfer problem grid to gpu.
          if(prob->cur_u_id != cur_u_id || prob->cur_v_id != cur_v_id)
          {
            cudaMemcpy(prob->gpuR, prob->R2D[cur_grid_id], sizeof(mf_node)*prob->gridSize[cur_grid_id], cudaMemcpyHostToDevice);
          }
          gpuErr(cudaPeekAtLastError());
          prob->cur_u_id = cur_u_id;
          prob->cur_v_id = cur_v_id;

          //transfer p grid to gpu
          if(model->cur_u_id == -1)
          {
            short *p_tmp = model->halfp + model->u_seg*model->k*cur_u_id; 
            cudaMemcpy(model->gpuHalfp, p_tmp, sizeof(half)*model->u_seg*model->k, cudaMemcpyHostToDevice);
            gpuErr(cudaPeekAtLastError());
          }
          else if(model->cur_u_id != cur_u_id)
          {
            short *p_tmp = model->halfp + model->u_seg*model->k*model->cur_u_id;
            cudaMemcpy(p_tmp, model->gpuHalfp, sizeof(half)*model->u_seg*model->k, cudaMemcpyDeviceToHost);
            gpuErr(cudaPeekAtLastError());

            p_tmp = model->halfp + model->u_seg*model->k*cur_u_id;
            cudaMemcpy(model->gpuHalfp, p_tmp, sizeof(half)*model->u_seg*model->k, cudaMemcpyHostToDevice);
            gpuErr(cudaPeekAtLastError());
          }
          model->cur_u_id = cur_u_id;
          gpuErr(cudaPeekAtLastError());

          //transfer q grid to gpu
          if(model->cur_v_id == -1)
          {
            short *q_tmp = model->halfq + model->v_seg*model->k*cur_v_id;
            cudaMemcpy(model->gpuHalfq, q_tmp, sizeof(half)*model->v_seg*model->k, cudaMemcpyHostToDevice);
            gpuErr(cudaPeekAtLastError());
          }
          else if(model->cur_v_id != cur_v_id)
          {
            short *q_tmp = model->halfq + model->v_seg*model->k*model->cur_v_id;
            cudaMemcpy(q_tmp, model->gpuHalfq, sizeof(half)*model->v_seg*model->k, cudaMemcpyDeviceToHost);
            gpuErr(cudaPeekAtLastError());

            q_tmp = model->halfq + model->v_seg*model->k*cur_v_id;
            cudaMemcpy(model->gpuHalfq, q_tmp, sizeof(half)*model->v_seg*model->k, cudaMemcpyHostToDevice);
            gpuErr(cudaPeekAtLastError());
          }
          model->cur_v_id = cur_v_id;
          gpuErr(cudaPeekAtLastError());

          //call the kernel
          sgd_k128_kernel_hogwild_warp32_lrate<<<para.num_workers/4,128>>>(
              prob->gpuR,
              prob->gridSize[cur_grid_id],
              model->gpuHalfp,
              model->gpuHalfq,
              rand_state,
              gpu_dynamic_rate,
              model->u_seg,
              model->v_seg,
              model->k,
              1,
              iter,
              max_update_count_per_block,
              update_count_per_block[cur_grid_id],
              update_vector_size,
              para.lambda_p,
              para.lambda_q,
              prob->u_grid,
              prob->v_grid,
              cur_u_id,
              cur_v_id);
          gpuErr(cudaPeekAtLastError());
        }
      }
      cudaDeviceSynchronize();

    }
    cudaDeviceSynchronize();
    printf("time elapsed:%.8fs\n",(clock()-start)/double(CLOCKS_PER_SEC));

    printf("%d,%d\n", model->cur_u_id, model->cur_v_id);
    //transfer p back to CPU
    if(model->cur_u_id >= 0)
    {
      short *p_tmp = model->halfp + model->u_seg*model->k*model->cur_u_id;
      cudaMemcpy(p_tmp, model->gpuHalfp, sizeof(half)*model->u_seg*model->k, cudaMemcpyDeviceToHost);
      gpuErr(cudaPeekAtLastError());
    }
    //transfer q back to CPU
    if(model->cur_v_id >= 0)
    {
      short *q_tmp = model->halfq + model->v_seg*model->k*model->cur_v_id;
      cudaMemcpy(q_tmp, model->gpuHalfq, sizeof(half)*model->v_seg*model->k, cudaMemcpyDeviceToHost);
      gpuErr(cudaPeekAtLastError());
    }
  }
  else
  {
    clock_t start = clock();

    //scheduling info
    int *global_x_list = new int[prob->x_grid*prob->y_grid];
    int *global_y_list = new int[prob->x_grid*prob->y_grid];
    int *global_id_list = new int[prob->x_grid*prob->y_grid];

    //create stream
    cudaStream_t stream_com, stream_mem_d2h, stream_mem_h2d;
    cudaStreamCreate(&stream_com);
    cudaStreamCreate(&stream_mem_d2h);
    cudaStreamCreate(&stream_mem_h2d);

    //random shuffle
    vector<int> u_id_vec(prob->u_grid, 0);
    vector<int> v_id_vec(prob->v_grid, 0);
    for(int i = 0;i < prob->u_grid;i++)u_id_vec[i] = i;
    for(int i = 0;i < prob->v_grid;i++)v_id_vec[i] = i;

    vector<int> x_id_vec(prob->x_grid, 0);
    vector<int> y_id_vec(prob->y_grid, 0);
    for(int i = 0;i < prob->x_grid;i++)x_id_vec[i] = i;
    for(int i = 0;i < prob->y_grid;i++)y_id_vec[i] = i;

    //fully random
    vector<int> uv_id_vec(prob->u_grid*prob->v_grid, 0);
    for(int i = 0;i < prob->u_grid*prob->v_grid; i++)uv_id_vec[i] = i;
    vector<int> xy_id_vec(prob->x_grid*prob->y_grid, 0);
    for(int i = 0;i < prob->x_grid*prob->y_grid; i++)xy_id_vec[i] = i;

    for(int iter = 0;iter < para.num_iters; iter ++)
    {
      fflush(stdout);
      clock_t ite_start = clock();

      shuffle(uv_id_vec.begin(), uv_id_vec.end(), g);
      shuffle(u_id_vec.begin(), u_id_vec.end(), g);

      for(int u_ite = 0;u_ite < prob->u_grid; u_ite ++)
      {
        shuffle(v_id_vec.begin(), v_id_vec.begin(), g);
        for(int v_ite = 0;v_ite < prob->v_grid; v_ite ++)
        {

          //fully random
          int tmp_uv_id = u_ite*prob->v_grid + v_ite;
          int cur_u_id = uv_id_vec[tmp_uv_id]/prob->v_grid;
          int cur_v_id = uv_id_vec[tmp_uv_id]%prob->v_grid;

          //set information
          shuffle(x_id_vec.begin(), x_id_vec.end(), g);
          shuffle(xy_id_vec.begin(), xy_id_vec.end(), g);

          for(int local_x_ite = 0;local_x_ite < prob->x_grid;local_x_ite ++)
          {
            shuffle(y_id_vec.begin(),y_id_vec.end(), g);
            for(int local_y_ite = 0;local_y_ite < prob->y_grid;local_y_ite ++)
            {

              //fully random
              int tmp_xy_id = local_x_ite*prob->y_grid + local_y_ite;
              int cur_x_id = xy_id_vec[tmp_xy_id]/prob->y_grid;
              int cur_y_id = xy_id_vec[tmp_xy_id]%prob->y_grid;

              int local_id = cur_x_id*prob->y_grid + cur_y_id;

              int global_x = cur_u_id*prob->x_grid + cur_x_id;
              int global_y = cur_v_id*prob->y_grid + cur_y_id;
              int global_id = global_x*prob->vy + global_y;

              global_x_list[local_id] = global_x;
              global_y_list[local_id] = global_y;
              global_id_list[local_id] = global_id;

            }
          }

          //run
          for(int i = -1;i < prob->x_grid*prob->y_grid;i++)
          {
            //compute
            if(i >= 0)
            {

              sgd_k128_kernel_hogwild_warp32_lrate<<<para.num_workers/4,128, 0, stream_com>>>(
                  prob->gpuRptrs[i%2],
                  prob->gridSize[global_id_list[i]],
                  model->gpuHalfPptrs[i%2],
                  model->gpuHalfQptrs[i%2],
                  rand_state,
                  gpu_dynamic_rate,
                  model->u_seg,
                  model->v_seg,
                  model->k,
                  1,
                  iter,
                  max_update_count_per_block,
                  update_count_per_block[global_id_list[i]],
                  update_vector_size,
                  para.lambda_p,
                  para.lambda_q,
                  prob->ux,
                  prob->vy,
                  global_x_list[i],
                  global_y_list[i]);
            }

            //memcpy for the next block
            if(i != (prob->x_grid*prob->y_grid - 1))
            {
              int next_global_x = global_x_list[i+1];
              int next_global_y = global_y_list[i+1];
              int next_global_id = global_id_list[i+1];

              //transfer problem grid to gpu
              if(prob->cur_global_x_id[(i+1)%2] !=  next_global_x || prob->cur_global_y_id[(i+1)%2] != next_global_y)
              {
                cudaMemcpyAsync(prob->gpuRptrs[(i+1)%2], 
                    prob->R2D[next_global_id], 
                    sizeof(mf_node)*prob->gridSize[next_global_id],
                    cudaMemcpyHostToDevice,
                    stream_mem_h2d);
              }

              //transfer feature p
              if(model->cur_global_x_id[(i+1)%2] == -1)
              {
                if(model->cur_global_x_id[(i+2)%2] == next_global_x)
                {
                  model->cur_global_x_id[(i+2)%2] = -1;
                  model->cur_global_x_id[(i+1)%2] = next_global_x;

                  half *tmp_ptr = model->gpuHalfPptrs[(i+1)%2];
                  model->gpuHalfPptrs[(i+1)%2] = model->gpuHalfPptrs[(i+2)%2];
                  model->gpuHalfPptrs[(i+2)%2] = tmp_ptr;
                }
                else
                {
                  short *p_tmp = model->halfp + model->u_seg*model->k*next_global_x;
                  cudaMemcpyAsync(model->gpuHalfPptrs[(i+1)%2],
                      p_tmp,    
                      sizeof(half)*model->u_seg*model->k,
                      cudaMemcpyHostToDevice,
                      stream_mem_h2d);
                  model->cur_global_x_id[(i+1)%2] = next_global_x;
                }
              }
              else if(model->cur_global_x_id[(i+1)%2] != next_global_x)
              {
                if(model->cur_global_x_id[(i+2)%2] == -1)
                {
                  //swap value
                  int tmp = model->cur_global_x_id[(i+1)%2];
                  model->cur_global_x_id[(i+1)%2] = next_global_x;
                  model->cur_global_x_id[(i+2)%2] = tmp;

                  //swap pointer
                  half *p_tmp = model->gpuHalfPptrs[(i+1)%2];
                  model->gpuHalfPptrs[(i+1)%2] = model->gpuHalfPptrs[(i+2)%2];
                  model->gpuHalfPptrs[(i+2)%2] = p_tmp;

                  //transfer
                  short *p_tmp_trans = model->halfp + model->u_seg*model->k*next_global_x;
                  cudaMemcpyAsync(model->gpuHalfPptrs[(i+1)%2],
                      p_tmp_trans,    
                      sizeof(half)*model->u_seg*model->k,
                      cudaMemcpyHostToDevice,
                      stream_mem_h2d);
                  model->cur_global_x_id[(i+1)%2] = next_global_x;
                }
                else if(model->cur_global_x_id[(i+2)%2] == next_global_x)
                {
                  //swap value
                  int tmp = model->cur_global_x_id[(i+1)%2];
                  model->cur_global_x_id[(i+1)%2] = next_global_x;
                  model->cur_global_x_id[(i+2)%2] = tmp;

                  //swap pointer
                  half *p_tmp = model->gpuHalfPptrs[(i+1)%2];
                  model->gpuHalfPptrs[(i+1)%2] = model->gpuHalfPptrs[(i+2)%2];
                  model->gpuHalfPptrs[(i+2)%2] = p_tmp;
                }
                else
                {
                  short *p_tmp = model->halfp + model->u_seg*model->k*model->cur_global_x_id[(i+1)%2];
                  cudaMemcpyAsync(p_tmp,
                      model->gpuHalfPptrs[(i+1)%2],
                      sizeof(half)*model->u_seg*model->k,
                      cudaMemcpyDeviceToHost,
                      stream_mem_d2h);

                  p_tmp = model->halfp + model->u_seg*model->k*next_global_x;
                  cudaMemcpyAsync(model->gpuHalfPptrs[(i+1)%2],
                      p_tmp,
                      sizeof(half)*model->u_seg*model->k,
                      cudaMemcpyHostToDevice,
                      stream_mem_h2d);

                  model->cur_global_x_id[(i+1)%2] = next_global_x;
                }
              }

              //transfer feature q
              if(model->cur_global_y_id[(i+1)%2] == -1)
              {
                if(model->cur_global_y_id[(i+2)%2] == next_global_y)
                {
                  model->cur_global_y_id[(i+2)%2] = -1;
                  model->cur_global_y_id[(i+1)%2] = next_global_y;

                  half *tmp_ptr = model->gpuHalfQptrs[(i+1)%2];
                  model->gpuHalfQptrs[(i+1)%2] = model->gpuHalfQptrs[(i+2)%2];
                  model->gpuHalfQptrs[(i+2)%2] = tmp_ptr;
                }
                else
                {
                  short *q_tmp = model->halfq + model->v_seg*model->k*next_global_y;
                  cudaMemcpyAsync(model->gpuHalfQptrs[(i+1)%2],
                      q_tmp,
                      sizeof(half)*model->v_seg*model->k,
                      cudaMemcpyHostToDevice,
                      stream_mem_h2d);
                  model->cur_global_y_id[(i+1)%2] = next_global_y;
                }
              }
              else if(model->cur_global_y_id[(i+1)%2] != next_global_y)
              {
                if(model->cur_global_y_id[(i+2)%2] == -1)
                {
                  //swap value
                  int tmp = model->cur_global_y_id[(i+1)%2];
                  model->cur_global_y_id[(i+1)%2] = model->cur_global_y_id[(i+2)%2];
                  model->cur_global_y_id[(i+2)%2] = tmp;

                  //swap pointer
                  half *q_tmp = model->gpuHalfQptrs[(i+1)%2];
                  model->gpuHalfQptrs[(i+1)%2] = model->gpuHalfQptrs[(i+2)%2];
                  model->gpuHalfQptrs[(i+2)%2] = q_tmp;

                  short *q_tmp_trans = model->halfq + model->v_seg*model->k*next_global_y;
                  cudaMemcpyAsync(model->gpuHalfQptrs[(i+1)%2],
                      q_tmp_trans,
                      sizeof(half)*model->v_seg*model->k,
                      cudaMemcpyHostToDevice,
                      stream_mem_h2d);
                  model->cur_global_y_id[(i+1)%2] = next_global_y;
                }
                else if(model->cur_global_y_id[(i+2)%2] == next_global_y)
                {
                  //swap value
                  int tmp = model->cur_global_y_id[(i+1)%2];
                  model->cur_global_y_id[(i+1)%2] = model->cur_global_y_id[(i+2)%2];
                  model->cur_global_y_id[(i+2)%2] = tmp;

                  //swap pointer
                  half *q_tmp = model->gpuHalfQptrs[(i+1)%2];
                  model->gpuHalfQptrs[(i+1)%2] = model->gpuHalfQptrs[(i+2)%2];
                  model->gpuHalfQptrs[(i+2)%2] = q_tmp;
                }
                else
                {
                  short *q_tmp = model->halfq + model->v_seg*model->k*model->cur_global_y_id[(i+1)%2];
                  cudaMemcpyAsync(q_tmp,
                      model->gpuHalfQptrs[(i+1)%2],
                      sizeof(half)*model->v_seg*model->k,
                      cudaMemcpyDeviceToHost,
                      stream_mem_d2h);

                  q_tmp = model->halfq + model->v_seg*model->k*next_global_y;
                  cudaMemcpyAsync(model->gpuHalfQptrs[(i+1)%2],
                      q_tmp,
                      sizeof(half)*model->v_seg*model->k,
                      cudaMemcpyHostToDevice,
                      stream_mem_h2d);
                  model->cur_global_y_id[(i+1)%2] = next_global_y;
                }
              }
            }
            cudaDeviceSynchronize();
          }   
        }
      }

      cudaDeviceSynchronize();
    }
    cudaDeviceSynchronize();
    printf("time elapsed:%.8fs\n",(clock()-start)/double(CLOCKS_PER_SEC));

    //transfer p back
    if(model->cur_global_x_id[0] != -1)
    {
      short *p_tmp = model->halfp + model->u_seg*model->k*model->cur_global_x_id[0];
      cudaMemcpy(p_tmp, model->gpuHalfPptrs[0], sizeof(half)*model->u_seg*model->k, cudaMemcpyDeviceToHost);
    }
    if(model->cur_global_x_id[1] != -1)
    {
      short *p_tmp = model->halfp + model->u_seg*model->k*model->cur_global_x_id[1];
      cudaMemcpy(p_tmp, model->gpuHalfPptrs[1], sizeof(half)*model->u_seg*model->k, cudaMemcpyDeviceToHost);
    }

    //transfer q back
    if(model->cur_global_y_id[0] != -1)
    {
      short *q_tmp = model->halfq + model->v_seg*model->k*model->cur_global_y_id[0];
      cudaMemcpy(q_tmp, model->gpuHalfQptrs[0], sizeof(half)*model->v_seg*model->k, cudaMemcpyDeviceToHost);
    }
    if(model->cur_global_y_id[1] != -1)
    {
      short *q_tmp = model->halfq + model->v_seg*model->k*model->cur_global_y_id[1];
      cudaMemcpy(q_tmp, model->gpuHalfQptrs[1], sizeof(half)*model->v_seg*model->k, cudaMemcpyDeviceToHost);
    }
  }   

  if(prob->x_grid*prob->y_grid == 1)
  {
    cudaFree(model->gpuHalfp);
    cudaFree(model->gpuHalfq);
    cudaFree(prob->gpuR);
  }
  else
  {
    cudaFree(model->gpuHalfPptrs[0]);
    cudaFree(model->gpuHalfPptrs[1]);
    cudaFree(model->gpuHalfQptrs[0]);
    cudaFree(model->gpuHalfQptrs[1]);
    cudaFree(prob->gpuRptrs[0]);
    cudaFree(prob->gpuRptrs[1]);
  }

  gpuErr(cudaPeekAtLastError());

  //transform halfp & halfq to floatp & floatq.
  cudaDeviceSynchronize();
  transform_feature_vector(model->halfp, model->floatp, model->m, model->ux, model->u_seg, model->k);
  transform_feature_vector(model->halfq, model->floatq, model->n, model->vy, model->v_seg, model->k);

  cudaFree(gpu_dynamic_rate);
  cudaFree(rand_state);
}
